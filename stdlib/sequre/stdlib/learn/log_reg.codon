""" Logistic regression module """

from numpy.ndarray import ndarray

from sequre.attributes import sequre
from sequre.stdlib.builtin import chebyshev_sigmoid, chebyshev_log
from sequre.settings import DEBUG
from sequre.mpc.env import MPCEnv

from sequre import MPU
from utils import batch


class LogReg[T]:
    coef_: T
    optimizer: str
    interval: tuple[float, float]

    def __init__(self):
        self.coef_ = T()
        self.optimizer = ""
        self.interval = (0.0, 0.0)
    
    def __init__(self, mpc: MPCEnv):
        self.coef_ = T(mpc) if isinstance(T, MPU) else T()
        self.optimizer = ""
        self.interval = (0.0, 0.0)
    
    def __init__(self, initial_weights: T, optimizer: str = "bgd", interval: tuple[float, float] = (-10.0, 10.0)):
        self.coef_ = initial_weights.copy()
        self.optimizer = optimizer
        self.interval = interval

    def fit(self, mpc, X: T, y: T, step: float, epochs: int, verbose: bool = False) -> LogReg[T]:
        self.coef_ = LogReg._fit(mpc, X, y, self.coef_, self.interval, step, epochs, self.optimizer, verbose)
        return self
    
    def predict(self, mpc, X: T, *args) -> T:
        return LogReg._predict(mpc, X, self.coef_, self.interval)
    
    def loss(self, mpc, X: T, y: T) -> T:
        return LogReg._loss(mpc, X, y, self.coef_, self.interval)

    def randomize_weights(self, mpc, distribution: str = "uniform"):
        self.coef_ = self.coef_.rand(distribution, mpc)
    
    def _fit(mpc, X: T, y: T, initial_w: T, interval: tuple[float, float], step: float, epochs: int, optimizer: str, verbose: bool, debug: Static[int] = DEBUG) -> T:
        # Adding bias
        X_tilde = X.pad_with_value(1, 1, 1, mpc)

        # Gradient descent
        if optimizer == "bgd":
            return LogReg._bgd(mpc, X_tilde, y, initial_w, interval, step, epochs, verbose, debug)
        if optimizer == "mbgd":
            return LogReg._mbgd(mpc, X_tilde, y, initial_w, interval, step, epochs, 10, verbose, debug)
        else:
            raise ValueError(f"LogReg: invalid optimizer passed: {optimizer}")
    
    @sequre
    def _bgd(mpc, X_tilde: T, y: T, initial_w: T, interval: tuple[float, float], step: float, epochs: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLog. reg. BGD step size:", step)
        
        # Batched gradient descent
        w = initial_w  # n x 1
        for _ in range(epochs):
            if verbose:
                print(f"CP{mpc.pid}:\tLog. reg. BGD epoch {_ + 1}/{epochs}")
            if debug:
                print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LogReg._loss(mpc, X_tilde, y, w, interval).reveal(mpc)}")
            
            dot = X_tilde @ w
            sig = chebyshev_sigmoid(mpc, dot, interval)  # m x 1
            w -= X_tilde.T @ (sig - y) * step  # n x 1
        
        return w
    
    @sequre
    def _mbgd(mpc, X_tilde: T, y: T, initial_w: T, interval: tuple[float, float], step: float, epochs: int, batches: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLog. reg. BGD step size:", step)
        
        # Compute mini-batches
        X_mini_batches = batch(mpc, X_tilde, batch_count=batches)
        y_mini_batches = batch(mpc, y, batch_count=batches)
        
        # Mini-batched gradient descent
        w = initial_w
        for _ in range(epochs):
            for i in range(batches):
                if verbose:
                    print(f"CP{mpc.pid}:\tLog. reg. MBGD epoch {_ + 1}/{epochs} -- batch {i + 1}/{batches}")
                if debug:
                    print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LogReg._loss(mpc, X_mini_batches[i], y_mini_batches[i], w, interval).reveal(mpc)}")
                
                dot = X_mini_batches[i] @ w
                sig = chebyshev_sigmoid(mpc, dot, interval)  # m x 1
                w -= X_mini_batches[i].T @ (sig - y_mini_batches[i]) * step
        
        return w
    
    @sequre
    def _predict(mpc, X: T, w: T, interval: tuple[float, float]) -> T:
        return chebyshev_sigmoid(mpc, (X.pad_with_value(1, 1, 1, mpc) @ w), interval)
    
    @sequre
    def _loss(mpc, X: T, y: T, w: T, interval: tuple[float, float]):
        a = chebyshev_sigmoid(mpc, X @ w, interval)
        return (-y * chebyshev_log(mpc, a, (0.0, 1.0)) - (1 - y) * chebyshev_log(mpc, 1 - a, (0.0, 1.0))).sum(axis=0)
