import math

from sequre.attributes import sequre
from sequre.decorators import flatten

from sequre.types.sharetensor import Sharetensor
from sequre.constants import MPC_NBIT_K, MPC_NBIT_F


def __normalizer_even_exponent(mpc, a, modulus):
    s_val, s_sqrt_val = mpc.fp.__nee_wrapper(a.share, modulus)
    s = Sharetensor(s_val, modulus)
    s_sqrt = Sharetensor(s_sqrt_val, modulus)
    return s, s_sqrt


@sequre
def __fp_mul_inv(mpc, b):
    if not b.fp:
        b = b.to_fp()

    modulus = b.modulus
    niter = int(2 * math.ceil(math.log2(MPC_NBIT_K / 3.5))) + 1

    denominator_sign = (b > 0) * 2 - 1
    b = b * denominator_sign
    
    s, _ = __normalizer_even_exponent(mpc, b, modulus)

    b_scaled = b * s
    b_scaled.share = mpc.fp.trunc(b_scaled.share, modulus, MPC_NBIT_K, MPC_NBIT_K - MPC_NBIT_F)

    scaled_est = b_scaled * b_scaled * 5 - b_scaled * 10 + 5.9430

    w = scaled_est * s
    w.share = mpc.fp.trunc(w.share, modulus, MPC_NBIT_K + MPC_NBIT_F + 2, MPC_NBIT_K - MPC_NBIT_F)

    x = 1 - w * b
    y = w

    # TODO: #32 Have Sequre inspect code downstream in order to figure out if partitions are necessary.
    for _ in range(niter):
        if not x.is_partitioned(): x.set_partitions(mpc.arithmetic.__beaver_partition(x.share, modulus))
        if not y.is_partitioned(): y.set_partitions(mpc.arithmetic.__beaver_partition(y.share, modulus))
        y = y * (x + 1)
        x = x * x
    
    # TODO: #32 Have Sequre inspect code downstream in order to figure out if partitions are necessary.
    if not x.is_partitioned(): x.set_partitions(mpc.arithmetic.__beaver_partition(x.share, modulus))
    if not y.is_partitioned(): y.set_partitions(mpc.arithmetic.__beaver_partition(y.share, modulus))

    return y * (x + 1) * denominator_sign


@sequre
def __fp_div(mpc, a, b):
    return __fp_mul_inv(mpc, b) * a


@sequre
def __fp_sqrt(mpc, a):
    modulus = a.modulus
    niter = int(2 * math.ceil(math.log2(MPC_NBIT_K / 3.5)))
    s, s_sqrt = __normalizer_even_exponent(mpc, a, modulus)

    a_scaled = a * s
    a_scaled.share = mpc.fp.trunc(a_scaled.share, modulus, MPC_NBIT_K, MPC_NBIT_K - MPC_NBIT_F)

    scaled_est = a_scaled * a_scaled * 2 - a_scaled * 4 + 2.9581

    h = scaled_est * s_sqrt
    h.share = mpc.fp.trunc(h.share, modulus, MPC_NBIT_K // 2 + MPC_NBIT_F + 2, (MPC_NBIT_K - MPC_NBIT_F) // 2 + 1)

    g = h * 2 * a
    for _ in range(niter):
        r = 1.5 - h * g
        # TODO: #16 Calculate h and g in parallel
        # r.get_partitions(mpc, force=True)
        h = h * r
        g = g * r
    
    return g, h * 2


@flatten(1)
def fp_div(mpc, a, b):
    modulus = b.modulus

    if modulus.popcnt() == 1:
        if isinstance(a, Sharetensor):
            a.to_field(mpc)
        b.to_field(mpc)
        
        res = __fp_div(mpc, a, b)
        res.to_ring(mpc)
        
        b.to_ring(mpc)
        if isinstance(a, Sharetensor):
            a.to_ring(mpc)
        return res
    return __fp_div(mpc, a, b)
    

@flatten(0)
def fp_sqrt(mpc, a):
    modulus = a.modulus

    if modulus.popcnt() == 1:
        a.to_field(mpc)
        res, res_inv = __fp_sqrt(mpc, a)
        res.to_ring(mpc)
        res_inv.to_ring(mpc)
        a.to_ring(mpc)
        return res, res_inv
    return __fp_sqrt(mpc, a)
