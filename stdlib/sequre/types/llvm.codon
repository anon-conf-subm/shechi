from sequre.constants import MPC_FIELD_SIZE, MPC_INT_SIZE, LATTISEQ_INT_SIZE, lattiseq_uint


# Const for optimized modulus operation
int_tt = UInt[MPC_INT_SIZE * 2]
# MOD_CONST = (int_tt(-1).gmp_floordiv(MPC_FIELD_SIZE.ext_double())) + int_tt(1)
MOD_CONST = int_tt(0)
if MPC_INT_SIZE == 128:
    MOD_CONST = int_tt("680564733841876926926749214863536422917")
elif MPC_INT_SIZE == 192:
    MOD_CONST = int_tt("1645504557321206042154969182557350504982735865633580069507039233")
elif MPC_INT_SIZE == 256:
    MOD_CONST = int_tt("3705346855594118253554271520278013051304639509300498049262642688253220148487169")
else:
    compile_error("Unsupported MPC int size. Should be 128, 192, or 256.")


@extend
class float:
    @llvm
    def bitcast_to_int(self: float) -> int:
        %0 = bitcast double %self to i64
        ret i64 %0


@extend
class int:
    @pure
    @llvm
    def __lhsr__(self, other: int) -> int:
        %0 = lshr i64 %self, %other
        ret i64 %0
    
    @llvm
    def bitcast_to_float(self: int) -> float:
        %0 = bitcast i64 %self to double
        ret double %0

    @llvm
    def bit_reverse(self: int) -> int:
        declare i64 @llvm.bitreverse.i64(i64)
        %0 = call i64 @llvm.bitreverse.i64(i64 %self)
        ret i64 %0
    
    @llvm
    def __naive_mod(self: int, other: int) -> int:
        declare i64 @llvm.ctpop.i64(i64)
        %popcount = call i64 @llvm.ctpop.i64(i64 %other)
        %0 = srem i64 %self, %other
        %1 = icmp slt i64 %0, 0
        %2 = add i64 %0, %other
        %3 = select i1 %1, i64 %2, i64 %0
        %4 = icmp eq i64 %popcount, 1
        %other_1 = sub i64 %other, 1
        %5 = and i64 %self, %other_1
        %6 = select i1 %4, i64 %5, i64 %3
        ret i64 %6

    @llvm
    def __fast_mod_const_field(self: int, field_size: int, mod_const: UInt[128]) -> int:
        %0 = lshr i64 %self, 63
        %1 = trunc i64 %0 to i1
        %2 = xor i64 %self, -1
        %3 = add i64 %2, 1
        %4 = select i1 %1, i64 %3, i64 %self
        %5 = zext i64 %4 to i128
        %6 = mul i128 %mod_const, %5
        %7 = zext i64 %field_size to i128
        %8 = zext i64 -1 to i128
        %9 = and i128 %6, %8
        %10 = mul i128 %9, %7
        %11 = lshr i128 %10, 64
        %12 = lshr i128 %6, 64
        %13 = mul i128 %12, %7
        %14 = add i128 %11, %13
        %15 = lshr i128 %14, 64
        %16 = trunc i128 %15 to i64
        %17 = sub i64 %field_size, %16
        %18 = select i1 %1, i64 %17, i64 %16
        ret i64 %18


@extend
class UInt:
    @llvm
    def _new_from_bit(other: UInt[1]) -> UInt[N]:
        %0 = zext i1 %other to i{=N}
        ret i{=N} %0
    
    @llvm
    def _new_from_bool(other: bool) -> UInt[N]:
        %0 = zext i8 %other to i{=N}
        ret i{=N} %0
    
    @llvm
    def from_float_to_intn(other: float) -> UInt[N]:
        %0 = fptoui double %other to i{=N}
        ret i{=N} %0

    @llvm
    def raw_add_overflow(self: UInt[N], other: UInt[N]) -> Tuple[UInt[N], UInt[1]]:
        declare {i{=N}, i1} @llvm.uadd.with.overflow.imag{=N}(i{=N}, i{=N})
        %res = call {i{=N}, i1} @llvm.uadd.with.overflow.imag{=N}(i{=N} %self, i{=N} %other)
        ret {i{=N}, i1} %res

    @llvm
    def bit_reverse(self: UInt[N]) -> UInt[N]:
        declare i{=N} @llvm.bitreverse.imag{=N}(i{=N})
        %0 = call i{=N} @llvm.bitreverse.imag{=N}(i{=N} %self)
        ret i{=N} %0

    @llvm
    def bitlen(self: UInt[N]) -> UInt[N]:
        declare i{=N} @llvm.ctlz.imag{=N}(i{=N}, i1)
        %0 = call i{=N} @llvm.ctlz.imag{=N}(i{=N} %self, i1 0)
        %1 = sub i{=N} {=N}, %0
        ret i{=N} %1
    
    @llvm
    def __naive_mod(self: UInt[N], other: UInt[N]) -> UInt[N]:
        %0 = urem i{=N} %self, %other
        ret i{=N} %0
    
    @llvm
    def __fast_mod_const_field(self: UInt[N], field_size: UInt[N] = MPC_FIELD_SIZE, mod_const: UInt[N*2] = MOD_CONST) -> UInt[N]:
        %0 = zext i{=N} %self to i{=N*2}
        %1 = mul i{=N*2} %mod_const, %0
        %2 = zext i{=N} %field_size to i{=N*2}
        %3 = zext i{=N} -1 to i{=N*2}
        %4 = and i{=N*2} %1, %3
        %5 = mul i{=N*2} %4, %2
        %6 = lshr i{=N*2} %5, {=N}
        %7 = lshr i{=N*2} %1, {=N}
        %8 = mul i{=N*2} %7, %2
        %9 = add i{=N*2} %6, %8
        %10 = lshr i{=N*2} %9, {=N}
        %11 = trunc i{=N*2} %10 to i{=N}
        ret i{=N} %11
    
    @llvm
    def __unsafe__truncated_mod(self: UInt[N], other: UInt[N]) -> UInt[N]:
        %0 = trunc i{=N} %self to i{=N//2}
        %1 = trunc i{=N} %other to i{=N//2}
        %2 = urem i{=N//2} %0, %1
        %3 = zext i{=N//2} %2 to i{=N}
        ret i{=N} %3

    @llvm
    def upcast(other: int) -> UInt[N]:
        %0 = zext i64 %other to i{=N}
        ret i{=N} %0
    
    @llvm
    def upcast(other: u64) -> UInt[N]:
        %0 = zext i64 %other to i{=N}
        ret i{=N} %0
    
    @llvm
    def trunc_half(self: UInt[N]) -> UInt[N // 2]:
        %0 = trunc i{=N} %self to i{=N//2}
        ret i{=N//2} %0
    
    @llvm
    def trunc_to(self: UInt[N], T: Static[int]) -> UInt[T]:
        %0 = trunc i{=N} %self to i{=T}
        ret i{=T} %0
    
    @llvm
    def ext_double(self: UInt[N]) -> UInt[N * 2]:
        %0 = zext i{=N} %self to i{=N*2}
        ret i{=N*2} %0
    
    @llvm
    def ext_to(self: UInt[N], T: Static[int]) -> UInt[T]:
        %0 = zext i{=N} %self to i{=T}
        ret i{=T} %0
    
    @llvm
    def ext_to_lattiseq_uint(self: UInt[N]) -> lattiseq_uint:
        %0 = zext i{=N} %self to i{=LATTISEQ_INT_SIZE}
        ret i{=LATTISEQ_INT_SIZE} %0


@extend
class Int:
    @llvm
    def trunc_to_i64(self: Int[N]) -> int:
        %0 = trunc i{=N} %self to i64
        ret i64 %0
    
    @llvm
    def half(self: Int[N]) -> Int[N]:
        %0 = lshr i{=N} %self, 1
        ret i{=N} %0
    
    @llvm
    def lshr(self: Int[N], other: Int[N]) -> Int[N]:
        %0 = lshr i{=N} %self, %other
        ret i{=N} %0
    
    @llvm
    def sign(self: Int[N]) -> Int[N]:
        %0 = lshr i{=N} %self, {=N - 1}
        %1 = xor i{=N} %0, 1
        %2 = mul i{=N} %1, 2
        %3 = sub i{=N} %2, 1
        ret i{=N} %3
